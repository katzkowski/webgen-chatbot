{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "from pathlib import Path\r\n",
    "from typing import Any, Dict, List, Tuple\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "from numpy.typing import ArrayLike, NDArray\r\n",
    "from dotenv import load_dotenv\r\n",
    "\r\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\r\n",
    "from keras.models import Model\r\n",
    "from keras.preprocessing.image import load_img\r\n",
    "\r\n",
    "# clustering and dimension reduction\r\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, MeanShift\r\n",
    "from sklearn.decomposition import PCA\r\n",
    "from sklearn.metrics import silhouette_score\r\n",
    "\r\n",
    "load_dotenv()\r\n",
    "\r\n",
    "DATA_PATH = Path(os.getenv(\"DATA_PATH\"))\r\n",
    "\r\n",
    "# only for .ipynb because relative imports don't work\r\n",
    "root_path = (DATA_PATH.parent) \r\n",
    "os.chdir(str(root_path))\r\n",
    "\r\n",
    "import src.database.db_connector as db"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# directory for dataset\r\n",
    "datadir = None\r\n",
    "\r\n",
    "# parameter for cluster amount\r\n",
    "k = None\r\n",
    "\r\n",
    "# database name for results\r\n",
    "db_name = \"clustering_db\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Connecting to database\r\n",
    "# exceptions intentionally crash\r\n",
    "cnx = db.connect_to_database(db_name)\r\n",
    "cursor = db.get_connection_cursor(cnx)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def load_dataset(name: str) -> Tuple[List[str], Model]:\r\n",
    "    \"\"\"Loads a local dataset with name `dataset_name` and the pretrained VGG16 model.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        dataset_name (str): the name of the dataset to be loaded\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        Tuple[List[str], Model]: (file list of dataset, pretrained model)\r\n",
    "    \"\"\"\r\n",
    "    global datadir\r\n",
    "    datadir = (\r\n",
    "        Path(DATA_PATH)\r\n",
    "        / \"datasets\"\r\n",
    "        / (\"dataset_\" + name + \"_root\")\r\n",
    "        / (\"dataset_\" + name)\r\n",
    "    )\r\n",
    "\r\n",
    "    images = []\r\n",
    "\r\n",
    "    with os.scandir(datadir) as files:\r\n",
    "        for file in files:\r\n",
    "            images.append(file.name)\r\n",
    "\r\n",
    "    # load pretrained VGG16 model\r\n",
    "    model = VGG16()\r\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\r\n",
    "    return (images, model)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def extract_features(file_name: str, model: Model) -> ArrayLike:\r\n",
    "    \"\"\"Extracts the features for an image file using the specified (pre-trained) model.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        file_name (str): the name of image file\r\n",
    "        model (Model): pretrained model for feature prediction\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        ArrayLike: predictions for the image\r\n",
    "    \"\"\"\r\n",
    "    # load the image as a 224x224 array\r\n",
    "    img = load_img(datadir / file_name, target_size=(224, 224))\r\n",
    "\r\n",
    "    # convert from 'PIL.Image.Image' to numpy array\r\n",
    "    img = np.array(img)\r\n",
    "\r\n",
    "    # reshape the data for the model reshape(num_of_samples, dim 1, dim 2, channels)\r\n",
    "    reshaped_img = img.reshape(1, 224, 224, 3)\r\n",
    "\r\n",
    "    # prepare image for model\r\n",
    "    imgx = preprocess_input(reshaped_img)\r\n",
    "\r\n",
    "    # get the feature vector using pre-trained model\r\n",
    "    features = model.predict(imgx, use_multiprocessing=True)\r\n",
    "    return features"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def preview_cluster(cluster_files: List[str], cluster_idx: int = 0, cluster_name: str = None) -> None:\r\n",
    "    \"\"\"Preview images of the specified cluster in a subplot.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        cluster_files (List[str]): all files belonging to cluster\r\n",
    "        cluster_idx (int, optional): index of the cluster\r\n",
    "    \"\"\"\r\n",
    "    cluster_len = len(cluster_files)\r\n",
    "\r\n",
    "    # only allow up to 10 images to be shown at a time\r\n",
    "    if len(cluster_files) > 10:\r\n",
    "        cluster_files = cluster_files[:10]\r\n",
    "\r\n",
    "    # show cluster name\r\n",
    "    ax = plt.subplot(k, 10 + 1,1+ max(cluster_idx, 0) * 11)\r\n",
    "    ax.axis(\"off\")\r\n",
    "    ax.text(0.3, 0.4, f\"{cluster_name}\\nlen: {cluster_len}\")\r\n",
    "\r\n",
    "    # plot each image in the cluster\r\n",
    "    for index, file in enumerate(cluster_files):\r\n",
    "        # calculate img index in plot\r\n",
    "        idx = index + 1 + 1 + (max(cluster_idx, 0) * 11)\r\n",
    "        img = load_img(datadir / file)\r\n",
    "\r\n",
    "        plt.subplot(k, 10 + 1, idx)\r\n",
    "        plt.imshow(img)\r\n",
    "        plt.axis(\"off\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def preview_all_clusters(clusters: Dict[Any, List[str]]) -> None:\r\n",
    "    \"\"\"Plot previews for all clusters.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        clusters (Dict[Any, List[str]]): dict of all clusters with file names\r\n",
    "    \"\"\"\r\n",
    "    plt.figure(figsize=(20 + 2, 2 * k))\r\n",
    "\r\n",
    "    # line number for plot\r\n",
    "    i = 0\r\n",
    "\r\n",
    "    for key in clusters:\r\n",
    "        preview_cluster(clusters[key], i, key)\r\n",
    "        i += 1\r\n",
    "    plt.ylabel(clusters.keys)\r\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_distribution(labels: NDArray, X: ArrayLike, centroids=None) -> None:\r\n",
    "    \"\"\"Plot the distribution of images as individuals as a scatter plot.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        labels (NDArray): predicted cluster for each image\r\n",
    "        X (ArrayLike): ndarray of shape (n_clusters, n_features)\r\n",
    "        centroids (NDArray, optional): ndarray of shape (n_clusters, n_features)\r\n",
    "    \"\"\"\r\n",
    "    # Get unique labels\r\n",
    "    unique_labels = np.unique(labels)\r\n",
    "\r\n",
    "    # plot the results\r\n",
    "    for i in unique_labels:\r\n",
    "        plt.scatter(X[labels == i, 0], X[labels == i, 1], label=i)\r\n",
    "    plt.legend()\r\n",
    "\r\n",
    "    # plot cluster centroids\r\n",
    "    if centroids is not None:\r\n",
    "        plt.scatter(centroids[:, 0], centroids[:, 1], s=80, color=\"k\")\r\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def store_clusters(\r\n",
    "    run_name: str,\r\n",
    "    k_value: int,\r\n",
    "    n_screenshots: int,\r\n",
    "    n_components: int,\r\n",
    "    clusters: Dict[str, List[str]]\r\n",
    ") -> None:\r\n",
    "    \"\"\"Store clusters in database.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        run_name (str): name of the clustering run\r\n",
    "        k_value (int): k-value of clustering run\r\n",
    "        n_screenshots (int): total number of screenshots\r\n",
    "        n_components (int): number of pca components\r\n",
    "        clusters (Dict[str, List[str]]): dict of clusters\r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    path_prefix = \"screenshots\\\\raw\\\\\"\r\n",
    "\r\n",
    "    try:\r\n",
    "        db.insert_clustering_run(db_name, run_name, k_value, n_screenshots, n_components, cursor)\r\n",
    "        cnx.commit()\r\n",
    "    except Exception:\r\n",
    "        cnx.rollback()\r\n",
    "        # return to avoid duplicates\r\n",
    "        return\r\n",
    "\r\n",
    "    for cluster_id in clusters:\r\n",
    "        # store cluster properties\r\n",
    "        try:\r\n",
    "            db.insert_cluster(db_name, str(cluster_id), run_name, k_value, len(clusters[cluster_id]), cursor)\r\n",
    "            cnx.commit()\r\n",
    "        except Exception:\r\n",
    "            cnx.rollback()\r\n",
    "            # do not continue, so more screenshots can be added to cluster\r\n",
    "        \r\n",
    "        # store file-to-cluster mappings\r\n",
    "        for filename in clusters[cluster_id]:\r\n",
    "            scr_id = db.get_screenshot_by_path(db_name, path_prefix + filename, cursor)\r\n",
    "\r\n",
    "            if scr_id is None:\r\n",
    "                continue\r\n",
    "            \r\n",
    "            try:\r\n",
    "                db.insert_cluster_assignment(db_name, str(cluster_id), run_name, k_value, scr_id[0], cursor)\r\n",
    "                cnx.commit()\r\n",
    "            except Exception:\r\n",
    "                cnx.rollback()\r\n",
    "                continue"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def init_training() -> Dict[str, ArrayLike]:\r\n",
    "    \"\"\"Prepare environment and data for training.\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        Dict[str, ArrayLike]: prepared training data\r\n",
    "    \"\"\"\r\n",
    "    # parameters\r\n",
    "    # dataset_name = \"v01_startups_clean\"\r\n",
    "    dataset_name = \"v01_busicorp\"\r\n",
    "    model_annotation = \"kmeans3_test\"\r\n",
    "\r\n",
    "    # load dataset and model\r\n",
    "    dataset, model = load_dataset(dataset_name, model_annotation)\r\n",
    "\r\n",
    "    data = {}\r\n",
    "\r\n",
    "    for image in dataset:\r\n",
    "        # try to extract the features and update the dictionary\r\n",
    "        try:\r\n",
    "            feat = extract_features(image, model)\r\n",
    "            data[image] = feat\r\n",
    "        # if something fails, save the extracted features as a pickle file (optional)\r\n",
    "        except Exception as err:\r\n",
    "            print(err)\r\n",
    "            # with open(p, \"wb\") as file:\r\n",
    "            #     pickle.dump(data, file)\r\n",
    "\r\n",
    "    return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = init_training()\r\n",
    "\r\n",
    "# get a list of the filenames\r\n",
    "filenames = np.array(list(data.keys()))\r\n",
    "\r\n",
    "# get a list of just the features\r\n",
    "feat = np.array(list(data.values()))\r\n",
    "\r\n",
    "# reshape so that there are 210 samples of 4096 vectors\r\n",
    "feat = feat.reshape(-1, 4096)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PCA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# reduce the amount of dimensions in the feature vector\r\n",
    "n_components = 20\r\n",
    "pca = PCA(n_components=n_components, random_state=22)\r\n",
    "pca.fit(feat)\r\n",
    "X_reduced = pca.transform(feat)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K-Means"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# cluster feature vectors\r\n",
    "# global k\r\n",
    "k = 25\r\n",
    "kmeans = KMeans(n_clusters=k, random_state=22, max_iter=300)\r\n",
    "kmeans_labels = kmeans.fit_predict(X_reduced)\r\n",
    "\r\n",
    "# sort clusters ascendingly \r\n",
    "kmeans_cluster_assignments = list(zip(filenames, kmeans_labels))\r\n",
    "kmeans_cluster_assignments.sort(key=lambda x: x[1])  \r\n",
    "\r\n",
    "# store filenames for each cluster\r\n",
    "kmeans_clusters = {}  # Dict[Any, List[str]]\r\n",
    "\r\n",
    "for file, cluster in kmeans_cluster_assignments:\r\n",
    "    if cluster not in kmeans_clusters.keys():\r\n",
    "        kmeans_clusters[cluster] = []\r\n",
    "        kmeans_clusters[cluster].append(file)\r\n",
    "    else:\r\n",
    "        kmeans_clusters[cluster].append(file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get cluster centroids\r\n",
    "centroids = kmeans.cluster_centers_\r\n",
    "\r\n",
    "plot_distribution(kmeans_labels, X_reduced, centroids)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# calculate cluster sizes\r\n",
    "kmeans_cluster_sizes = np.bincount(list(map(lambda tpl: tpl[1], kmeans_cluster_assignments)))\r\n",
    "\r\n",
    "plt.xlabel(\"clusters\")\r\n",
    "plt.ylabel(\"size\")\r\n",
    "\r\n",
    "bars = plt.bar(list(range(0,len(kmeans_cluster_sizes))), kmeans_cluster_sizes)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# plot preview for all\r\n",
    "preview_all_clusters(kmeans_clusters)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agglomerative Clustering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# cluster feature vectors\r\n",
    "agglo = AgglomerativeClustering(n_clusters=k) # 6\r\n",
    "agglo_labels = agglo.fit_predict(X_reduced)\r\n",
    "\r\n",
    "# sort clusters ascendingly \r\n",
    "agglo_cluster_assignments = list(zip(filenames, agglo_labels))\r\n",
    "agglo_cluster_assignments.sort(key=lambda x: x[1])  \r\n",
    "\r\n",
    "# store filenames for each cluster\r\n",
    "agglo_clusters = {}  # Dict[Any, List[str]]\r\n",
    "\r\n",
    "for file, cluster in agglo_cluster_assignments:\r\n",
    "    if cluster not in agglo_clusters.keys():\r\n",
    "        agglo_clusters[cluster] = []\r\n",
    "        agglo_clusters[cluster].append(file)\r\n",
    "    else:\r\n",
    "        agglo_clusters[cluster].append(file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get cluster centroids\r\n",
    "# centroids = agglo.cluster_centers_\r\n",
    "\r\n",
    "plot_distribution(agglo_labels, X_reduced)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# calculate cluster sizes\r\n",
    "agglo_cluster_sizes = np.bincount(list(map(lambda tpl: tpl[1], agglo_cluster_assignments)))\r\n",
    "\r\n",
    "plt.xlabel(\"clusters\")\r\n",
    "plt.ylabel(\"size\")\r\n",
    "\r\n",
    "plt.bar(list(range(0,len(agglo_cluster_sizes))), agglo_cluster_sizes)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# plot preview for all\r\n",
    "preview_all_clusters(agglo_clusters)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Meanshift Clustering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# cluster feature vectors\r\n",
    "mshift = MeanShift(bandwidth=45)\r\n",
    "mshift_labels = mshift.fit_predict(X_reduced)\r\n",
    "\r\n",
    "# sort clusters ascendingly \r\n",
    "mshift_cluster_assignments = list(zip(filenames, mshift_labels))\r\n",
    "mshift_cluster_assignments.sort(key=lambda x: x[1])  \r\n",
    "\r\n",
    "# store filenames for each cluster\r\n",
    "mshift_clusters = {}  # Dict[Any, List[str]]\r\n",
    "\r\n",
    "for file, cluster in mshift_cluster_assignments:\r\n",
    "    if cluster not in mshift_clusters.keys():\r\n",
    "        mshift_clusters[cluster] = []\r\n",
    "        mshift_clusters[cluster].append(file)\r\n",
    "    else:\r\n",
    "        mshift_clusters[cluster].append(file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get cluster centroids\r\n",
    "centroids = mshift.cluster_centers_\r\n",
    "\r\n",
    "plot_distribution(mshift_labels, X_reduced, centroids)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# calculate cluster sizes\r\n",
    "mshift_cluster_sizes = np.bincount(list(map(lambda tpl: tpl[1], mshift_cluster_assignments)))\r\n",
    "\r\n",
    "plt.xlabel(\"clusters\")\r\n",
    "plt.ylabel(\"size\")\r\n",
    "\r\n",
    "bars = plt.bar(list(range(0,len(mshift_cluster_sizes))), mshift_cluster_sizes)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# plot preview for all\r\n",
    "preview_all_clusters(mshift_clusters)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# this is just incase you want to see which value for k might be the best\r\n",
    "sse = []\r\n",
    "sil = []\r\n",
    "list_k = list(range(2, 20, 1))\r\n",
    "# list_k = [10]\r\n",
    "\r\n",
    "for k_var in list_k:\r\n",
    "    km = KMeans(n_clusters=k_var, random_state=22, n_init=20)\r\n",
    "    km.fit(X_reduced)\r\n",
    "\r\n",
    "    # within cluster sum of squared errors\r\n",
    "    sse.append(km.inertia_)\r\n",
    "\r\n",
    "    # sihloutte method\r\n",
    "    sil.append(silhouette_score(X_reduced, km.labels_, metric = 'euclidean'))\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Within Cluster Sum of Squared Errors (WSS)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot sse against k\r\n",
    "plt.figure(figsize=(6, 6))\r\n",
    "plt.plot(list_k, sse)\r\n",
    "plt.xlabel(r\"Number of clusters *k*\")\r\n",
    "plt.ylabel(\"Sum of squared distance\")\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Silhouette Score"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot sil against k\r\n",
    "plt.figure(figsize=(6, 6))\r\n",
    "plt.plot(list_k, sil)\r\n",
    "plt.xlabel(r\"Number of clusters *k*\")\r\n",
    "plt.ylabel(\"Silhouette score\")\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Store Clusters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "store_clusters(\"kmeans3_test_v01_busicorp\",25, len(filenames), n_components, kmeans_clusters)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ba-code",
   "language": "python",
   "name": "ba-code"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}