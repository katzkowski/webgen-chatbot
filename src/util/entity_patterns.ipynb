{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Patterns from Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Union\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "load_dotenv()\n",
    "DATA_PATH = Path(os.getenv(\"DATA_PATH\"))\n",
    "\n",
    "# only for .ipynb because relative imports don't work\n",
    "root_path = Path(DATA_PATH).parent\n",
    "os.chdir(str(root_path))\n",
    "\n",
    "import src.database.db_connector as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database for labels\n",
    "db_name = \"clustering_db\"\n",
    "cnx = db.connect_to_database(db_name)\n",
    "\n",
    "# bool to indicate df overwriting\n",
    "mapped = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained english pipeline\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions for pattern generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequences(lst: List[any]) -> List[List[any]]:\n",
    "    \"\"\"Get all subsequences of a list.\n",
    "\n",
    "    Args:\n",
    "        lst (List[any]): base list\n",
    "\n",
    "    Returns:\n",
    "        List[List[any]]: list of all list subsequences\n",
    "    \"\"\"    \n",
    "    # filter stop words\n",
    "    lst = [seq for seq in lst if seq not in nlp.Defaults.stop_words]\n",
    "\n",
    "    sequences = []\n",
    "    for k in range(0, len(lst)):\n",
    "        sequences += [lst[k:i] for i in range(k+1, len(lst)+1)]\n",
    "\n",
    "    # add elements with changed order for 2-tuples\n",
    "    sequences += [[elem[1], elem[0]] for elem in sequences if len(elem) == 2]\n",
    "\n",
    "    sequences.sort(key=len)\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def get_pattern(tuple: Tuple[str, str, str]) -> List[Dict[str, Union[str, List[Dict[str, str]]]]]:\n",
    "    \"\"\"Creates a pattern for the spaCy EntityRuler from a given label.\n",
    "\n",
    "    Args:\n",
    "        tuple (Tuple[str, str]): (label name, label type, label id)\n",
    "\n",
    "    Returns:\n",
    "         List[Dict[str, Union[str, List[Dict[str, str]]]]]: list of patterns\n",
    "            Either\n",
    "            \n",
    "            1. `{'label': 'CAT', 'pattern': [{'LOWER': 'promotional'}], 'id': 'Promotional'}` or\n",
    "\n",
    "            2. `{'label': 'CAT', 'pattern': [{'LOWER': 'culture'}, {'LOWER': 'education'}], 'id': 'Culture & Education'}`\n",
    "    \"\"\"    \n",
    "    labels = {\"category\": \"CAT\", \"color\": \"COLOR\", \"feature\": \"FEAT\"}\n",
    "\n",
    "    # create lowercase pattern \n",
    "    pattern = [{\"LOWER\": it.lower()} for it in tuple[1]]\n",
    "\n",
    "    return {\"label\": labels[tuple[0]], \"pattern\": pattern, \"id\": tuple[2]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    select l.name, l.type\n",
    "    from screenshots as s\n",
    "    inner join websites as w on w.url=s.page_url\n",
    "    inner join website_labels as wl on w.id=wl.website_id\n",
    "    inner join labels as l on wl.label_id=l.id\n",
    "    where l.type != \"technology\"\n",
    "    group by l.name\n",
    "    having count(*) > 100\n",
    "    order by l.type;\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(query, cnx)\n",
    "\n",
    "# rename type 'tag' to 'feature'\n",
    "df[\"type\"] = df[\"type\"].apply(lambda l_type: l_type if l_type != \"tag\" else \"feature\")\n",
    "\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label remapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restructure for remapping}\n",
    "base_mapping_dict = {name: {\"name\": name, \"type\": type } for (name, type) in df.values}\n",
    "\n",
    "print(base_mapping_dict)\n",
    "\n",
    "with open(DATA_PATH / \"chatbot\" / \"mappings_base.json\", \"w+\") as json_file:\n",
    "    json.dump(base_mapping_dict, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load manually edited mappings\n",
    "with open(DATA_PATH / \"chatbot\" / \"mappings.json\", \"r\") as json_file:\n",
    "    mappings_dict = json.load(json_file)\n",
    "\n",
    "# print(mappings_dict)\n",
    "\n",
    "new_df = pd.DataFrame.from_records([mappings_dict[key] for key in mappings_dict])\n",
    "\n",
    "# drop Nonetype rows\n",
    "new_df = new_df.dropna(how='any',axis=0) \n",
    "\n",
    "# overwrite old \n",
    "df = new_df\n",
    "mapped = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = df.to_dict(\"records\")\n",
    "\n",
    "# tokenize label names\n",
    "tuples = [\n",
    "    (\n",
    "        dct[\"type\"],\n",
    "        [\n",
    "            t.text\n",
    "            for t in nlp(dct[\"name\"], disable=[\"parser\", \"ner\"]) # only tokenize\n",
    "            if t.text is not None and t.text not in [\"/\", \"&\", \"-\"] # remove special tokens\n",
    "        ],\n",
    "        dct[\"name\"],\n",
    "    )\n",
    "    for dct in records\n",
    "]\n",
    "\n",
    "# get all subsequences from label name tokens\n",
    "more_tuples = [\n",
    "    (tpl[0], token, tpl[2]) for tpl in tuples for token in subsequences(tpl[1])\n",
    "]\n",
    "\n",
    "# ner patterns from tuples\n",
    "patterns = list(map(get_pattern, more_tuples))\n",
    "for pat in patterns:\n",
    "    print(pat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add entity ruler to pipeline\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# example request\n",
    "request = \"Create a promotional e colorful website for Google\"\n",
    "\n",
    "doc = nlp(request) \n",
    "print([token.text for token in doc])\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite_old_patterns = False\n",
    "\n",
    "print(mapped)\n",
    "\n",
    "# save patterns in file\n",
    "if mapped and overwrite_old_patterns: \n",
    "    ruler.to_disk(root_path / \"data\" / \"chatbot\" / \"auto_mapped_patterns.jsonl\")\n",
    "elif overwrite_old_patterns:\n",
    "    ruler.to_disk(root_path / \"data\" / \"chatbot\" / \"auto_patterns.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH / \"chatbot\" / \"colors.csv\")\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_uppercase(color_name):\n",
    "    return re.findall('[A-Z][^A-Z]*', color_name)\n",
    "\n",
    "# split names on uppercase and lower result\n",
    "df[\"name\"] = df[\"name\"].apply(lambda x: \" \".join(split_uppercase(x)).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = df.to_dict(\"records\")\n",
    "\n",
    "def to_pattern(color_dict: Dict[str, str]):\n",
    "    return {\"label\": \"HEX\", \"pattern\": [{\"LOWER\": color_dict[\"name\"]}], \"id\": color_dict[\"hex\"]}\n",
    "\n",
    "# create patterns from colors \n",
    "color_patterns = list(map(lambda c: to_pattern(c), colors))\n",
    "\n",
    "# print(color_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "# load pretrained pipeline only to get patterns\n",
    "colors_nlp = spacy.load(\"en_core_web_sm\")\n",
    "ruler = EntityRuler(colors_nlp)\n",
    "\n",
    "ruler.add_patterns(color_patterns)\n",
    "\n",
    "# save patterns in file\n",
    "# ruler.to_disk(root_path / \"data\" / \"chatbot\" / \"color_patterns.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07e3a7355d7abb8aa405eb35da3e7fd4ab2bc4b84e1079a2d318e7f47c8b0cf5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
