{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Entity Patterns from Labels"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "import json\r\n",
    "from pathlib import Path\r\n",
    "from typing import List, Tuple, Dict, Union\r\n",
    "\r\n",
    "import spacy\r\n",
    "import pandas as pd\r\n",
    "from dotenv import load_dotenv\r\n",
    "\r\n",
    "nlp = spacy.load(\"en_core_web_sm\")\r\n",
    "load_dotenv()\r\n",
    "DATA_PATH = Path(os.getenv(\"DATA_PATH\"))\r\n",
    "\r\n",
    "# only for .ipynb because relative imports don't work\r\n",
    "root_path = Path(DATA_PATH).parent\r\n",
    "os.chdir(str(root_path))\r\n",
    "\r\n",
    "import src.database.db_connector as db"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# database for labels\r\n",
    "db_name = \"clustering_db\"\r\n",
    "cnx = db.connect_to_database(db_name)\r\n",
    "\r\n",
    "# bool to indicate df overwriting\r\n",
    "mapped = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load pretrained english pipeline\r\n",
    "nlp = spacy.load(\"en_core_web_sm\")\r\n",
    "print(nlp.pipe_names)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utility functions for pattern generation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def subsequences(lst: List[any]) -> List[List[any]]:\r\n",
    "    \"\"\"Get all subsequences of a list.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        lst (List[any]): base list\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        List[List[any]]: list of all list subsequences\r\n",
    "    \"\"\"    \r\n",
    "    # filter stop words\r\n",
    "    lst = [seq for seq in lst if seq not in nlp.Defaults.stop_words]\r\n",
    "\r\n",
    "    sequences = []\r\n",
    "    for k in range(0, len(lst)):\r\n",
    "        sequences += [lst[k:i] for i in range(k+1, len(lst)+1)]\r\n",
    "\r\n",
    "    # add elements with changed order for 2-tuples\r\n",
    "    sequences += [[elem[1], elem[0]] for elem in sequences if len(elem) == 2]\r\n",
    "\r\n",
    "    sequences.sort(key=len)\r\n",
    "    return sequences\r\n",
    "\r\n",
    "\r\n",
    "def get_pattern(tuple: Tuple[str, str, str]) -> List[Dict[str, Union[str, List[Dict[str, str]]]]]:\r\n",
    "    \"\"\"Creates a pattern for the spaCy EntityRuler from a given label.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        tuple (Tuple[str, str]): (label name, label type, label id)\r\n",
    "\r\n",
    "    Returns:\r\n",
    "         List[Dict[str, Union[str, List[Dict[str, str]]]]]: list of patterns\r\n",
    "            Either\r\n",
    "            \r\n",
    "            1. `{'label': 'CAT', 'pattern': [{'LOWER': 'promotional'}], 'id': 'Promotional'}` or\r\n",
    "\r\n",
    "            2. `{'label': 'CAT', 'pattern': [{'LOWER': 'culture'}, {'LOWER': 'education'}], 'id': 'Culture & Education'}`\r\n",
    "    \"\"\"    \r\n",
    "    labels = {\"category\": \"CAT\", \"color\": \"COLOR\", \"feature\": \"FEAT\"}\r\n",
    "\r\n",
    "    # create lowercase pattern \r\n",
    "    pattern = [{\"LOWER\": it.lower()} for it in tuple[1]]\r\n",
    "\r\n",
    "    return {\"label\": labels[tuple[0]], \"pattern\": pattern, \"id\": tuple[2]}\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "query = \"\"\"\r\n",
    "    select l.name, l.type\r\n",
    "    from screenshots as s\r\n",
    "    inner join websites as w on w.url=s.page_url\r\n",
    "    inner join website_labels as wl on w.id=wl.website_id\r\n",
    "    inner join labels as l on wl.label_id=l.id\r\n",
    "    where l.type != \"technology\"\r\n",
    "    group by l.name\r\n",
    "    having count(*) > 100\r\n",
    "    order by l.type;\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "df = pd.read_sql(query, cnx)\r\n",
    "\r\n",
    "# rename type 'tag' to 'feature'\r\n",
    "df[\"type\"] = df[\"type\"].apply(lambda l_type: l_type if l_type != \"tag\" else \"feature\")\r\n",
    "\r\n",
    "print(df.to_string())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Label remapping"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# restructure for remapping}\r\n",
    "base_mapping_dict = {name: {\"name\": name, \"type\": type } for (name, type) in df.values}\r\n",
    "\r\n",
    "print(base_mapping_dict)\r\n",
    "\r\n",
    "with open(DATA_PATH / \"chatbot\" / \"mappings_base.json\", \"w+\") as json_file:\r\n",
    "    json.dump(base_mapping_dict, json_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load manually edited mappings\r\n",
    "with open(DATA_PATH / \"chatbot\" / \"mappings.json\", \"r\") as json_file:\r\n",
    "    mappings_dict = json.load(json_file)\r\n",
    "\r\n",
    "# print(mappings_dict)\r\n",
    "\r\n",
    "new_df = pd.DataFrame.from_records([mappings_dict[key] for key in mappings_dict])\r\n",
    "\r\n",
    "# drop Nonetype rows\r\n",
    "new_df = new_df.dropna(how='any',axis=0) \r\n",
    "\r\n",
    "# overwrite old \r\n",
    "df = new_df\r\n",
    "mapped = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pattern generation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "records = df.to_dict(\"records\")\r\n",
    "\r\n",
    "# tokenize label names\r\n",
    "tuples = [\r\n",
    "    (\r\n",
    "        dct[\"type\"],\r\n",
    "        [\r\n",
    "            t.text\r\n",
    "            for t in nlp(dct[\"name\"], disable=[\"parser\", \"ner\"]) # only tokenize\r\n",
    "            if t.text is not None and t.text not in [\"/\", \"&\", \"-\"] # remove special tokens\r\n",
    "        ],\r\n",
    "        dct[\"name\"],\r\n",
    "    )\r\n",
    "    for dct in records\r\n",
    "]\r\n",
    "\r\n",
    "# get all subsequences from label name tokens\r\n",
    "more_tuples = [\r\n",
    "    (tpl[0], token, tpl[2]) for tpl in tuples for token in subsequences(tpl[1])\r\n",
    "]\r\n",
    "\r\n",
    "# ner patterns from tuples\r\n",
    "patterns = list(map(get_pattern, more_tuples))\r\n",
    "for pat in patterns:\r\n",
    "    print(pat)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pattern testing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# add entity ruler to pipeline\r\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\r\n",
    "ruler.add_patterns(patterns)\r\n",
    "\r\n",
    "# example request\r\n",
    "request = \"Create a promotional e colorful website for Google\"\r\n",
    "\r\n",
    "doc = nlp(request) \r\n",
    "print([token.text for token in doc])\r\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save patterns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "overwrite_old_patterns = False\r\n",
    "\r\n",
    "print(mapped)\r\n",
    "\r\n",
    "# save patterns in file\r\n",
    "if mapped and overwrite_old_patterns: \r\n",
    "    ruler.to_disk(root_path / \"data\" / \"chatbot\" / \"auto_mapped_patterns.jsonl\")\r\n",
    "elif overwrite_old_patterns:\r\n",
    "    ruler.to_disk(root_path / \"data\" / \"chatbot\" / \"auto_patterns.jsonl\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Color Patterns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.read_csv(DATA_PATH / \"chatbot\" / \"colors.csv\")\r\n",
    "\r\n",
    "print(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\r\n",
    "\r\n",
    "def split_uppercase(color_name):\r\n",
    "    return re.findall('[A-Z][^A-Z]*', color_name)\r\n",
    "\r\n",
    "# split names on uppercase and lower result\r\n",
    "df[\"name\"] = df[\"name\"].apply(lambda x: \" \".join(split_uppercase(x)).lower())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "colors = df.to_dict(\"records\")\r\n",
    "\r\n",
    "def to_pattern(color_dict: Dict[str, str]):\r\n",
    "    return {\"label\": \"HEX\", \"pattern\": [{\"LOWER\": color_dict[\"name\"]}], \"id\": color_dict[\"hex\"]}\r\n",
    "\r\n",
    "# create patterns from colors \r\n",
    "color_patterns = list(map(lambda c: to_pattern(c), colors))\r\n",
    "\r\n",
    "# print(color_patterns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from spacy.pipeline import EntityRuler\r\n",
    "\r\n",
    "# load pretrained pipeline only to get patterns\r\n",
    "colors_nlp = spacy.load(\"en_core_web_sm\")\r\n",
    "ruler = EntityRuler(colors_nlp)\r\n",
    "\r\n",
    "ruler.add_patterns(color_patterns)\r\n",
    "\r\n",
    "# save patterns in file\r\n",
    "# ruler.to_disk(root_path / \"data\" / \"chatbot\" / \"color_patterns.jsonl\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "07e3a7355d7abb8aa405eb35da3e7fd4ab2bc4b84e1079a2d318e7f47c8b0cf5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}